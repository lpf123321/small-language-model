# model config
vocab_size: 10000
context_length: 256
d_model: 512
d_ff: 1344
RoPE_theta: 10000
num_layers: 4
num_heads: 16

# training config
iterations: 40000 #  total tokens processed 327,680,000 (batch size × total step count × context length)
batch_size: 64
beta1: 0.9
beta2: 0.999
weight_decay: 0.01
max_learning_rate: 0.0003
min_learning_rate: 0.00003
warmup_iters: 1000
cosine_cycle_iters: 10000
gradient_clip_norm: 1.0
save_interval: 1000
eval_interval: 500
checkpoint_dir: "checkpoints"

# inference config
temperature: 0.85
top_p: 0.9
repetition_penalty: 1.2
max_len: 500